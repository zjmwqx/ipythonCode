{
 "metadata": {
  "name": "",
  "signature": "sha256:1d6ed63ae6bdb2756b22d1eda24949839a2a8dbae87a68fcc4c346fb8d7742bd"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import re\n",
      "import regex\n",
      "import json\n",
      "import mysql.connector\n",
      "import pandas.io.sql as psql\n",
      "import urllib\n",
      "import boto\n",
      "import boto.s3.connection\n",
      "import pickle\n",
      "import jpype\n",
      "import re\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "import random"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "db2 = mysql.connector.connect(host='security02-dev.datayes.com', db='securityMaster2', user='news_app', \n",
      "                              passwd='lKTOAIyoewzvCyc')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "selectSql = \"select ins.PARTY_ID, equ.TICKER_SYMBOL, ins.REGISTER_FULL_NAME, sec.SECURITY_NAME_ABBR\";\n",
      "selectSql += \" from INSTITUTION ins, SECURITY sec, EQUITY equ\";\n",
      "selectSql += \" where sec.ASSET_CLASS=3\";\n",
      "selectSql += \" and sec.PARTY_ID = ins.PARTY_ID\";\n",
      "selectSql += \" and equ.EQUITY_TYPE in(1,2)\";\n",
      "selectSql += \" and equ.SECURITY_ID = sec.SECURITY_ID\";"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "companyInfo = psql.read_sql(selectSql,db2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "db = mysql.connector.connect(\n",
      "host='db-news01-dev.datayes.com', db='news', user='news_app', passwd='lKTOAIyoewzvCyc')\n",
      "table_name = 'news_detail_backup'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dateSt = '2014-04-01'\n",
      "dateEd = '2014-07-01'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sql = 'select news_id, news_title,news_body,news_publish_time,main_related_company_id from %s where main_related_company_id != -1 and isActive = 1 and group_id != -1 and news_tag = 0 and news_publish_time > \"'+dateSt+'\" and news_publish_time < \"'+dateEd+'\";'\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = psql.read_sql(sql % table_name, db)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.to_pickle(data, 'news_' + dateSt+'_'+dateEd+'.ser')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = pd.read_pickle('news_' + dateSt+'_'+dateEd+'.ser')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "companyInfo.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>PARTY_ID</th>\n",
        "      <th>TICKER_SYMBOL</th>\n",
        "      <th>REGISTER_FULL_NAME</th>\n",
        "      <th>SECURITY_NAME_ABBR</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 2</td>\n",
        "      <td> 000001</td>\n",
        "      <td>     \u5e73\u5b89\u94f6\u884c\u80a1\u4efd\u6709\u9650\u516c\u53f8</td>\n",
        "      <td> \u5e73\u5b89\u94f6\u884c</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 3</td>\n",
        "      <td> 000002</td>\n",
        "      <td>     \u4e07\u79d1\u4f01\u4e1a\u80a1\u4efd\u6709\u9650\u516c\u53f8</td>\n",
        "      <td>  \u4e07\u79d1\uff21</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 3</td>\n",
        "      <td> 200002</td>\n",
        "      <td>     \u4e07\u79d1\u4f01\u4e1a\u80a1\u4efd\u6709\u9650\u516c\u53f8</td>\n",
        "      <td>  \u4e07\u79d1\uff22</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 5</td>\n",
        "      <td> 000004</td>\n",
        "      <td> \u6df1\u5733\u4e2d\u56fd\u519c\u5927\u79d1\u6280\u80a1\u4efd\u6709\u9650\u516c\u53f8</td>\n",
        "      <td> \u56fd\u519c\u79d1\u6280</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 6</td>\n",
        "      <td> 000005</td>\n",
        "      <td>   \u6df1\u5733\u4e16\u7eaa\u661f\u6e90\u80a1\u4efd\u6709\u9650\u516c\u53f8</td>\n",
        "      <td> \u4e16\u7eaa\u661f\u6e90</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 4 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "   PARTY_ID TICKER_SYMBOL REGISTER_FULL_NAME SECURITY_NAME_ABBR\n",
        "0         2        000001         \u5e73\u5b89\u94f6\u884c\u80a1\u4efd\u6709\u9650\u516c\u53f8               \u5e73\u5b89\u94f6\u884c\n",
        "1         3        000002         \u4e07\u79d1\u4f01\u4e1a\u80a1\u4efd\u6709\u9650\u516c\u53f8                \u4e07\u79d1\uff21\n",
        "2         3        200002         \u4e07\u79d1\u4f01\u4e1a\u80a1\u4efd\u6709\u9650\u516c\u53f8                \u4e07\u79d1\uff22\n",
        "3         5        000004     \u6df1\u5733\u4e2d\u56fd\u519c\u5927\u79d1\u6280\u80a1\u4efd\u6709\u9650\u516c\u53f8               \u56fd\u519c\u79d1\u6280\n",
        "4         6        000005       \u6df1\u5733\u4e16\u7eaa\u661f\u6e90\u80a1\u4efd\u6709\u9650\u516c\u53f8               \u4e16\u7eaa\u661f\u6e90\n",
        "\n",
        "[5 rows x 4 columns]"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "companyInfoMap = companyInfo.values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "companyInfoMap[companyInfoMap[:,0]==3][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "array([3, u'000002',\n",
        "       u'\\u4e07\\u79d1\\u4f01\\u4e1a\\u80a1\\u4efd\\u6709\\u9650\\u516c\\u53f8',\n",
        "       u'\\u4e07\\u79d1\\uff21'], dtype=object)"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def replace_name(news):\n",
      "    if news.main_related_company_id == -1:\n",
      "        return news.news_title\n",
      "    cpInfo = companyInfoMap[companyInfoMap[:,0]==news.main_related_company_id][0]\n",
      "    name = cpInfo[2]\n",
      "    stname = cpInfo[3]\n",
      "    sttname = cpInfo[3][0:2]\n",
      "    title = news.news_title\n",
      "    title = title.replace(name,'')\n",
      "    title = title.replace(stname,'')\n",
      "    title = title.replace(sttname,'')\n",
      "    return title    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data.news_title = data.apply(replace_name, axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data.main_related_company_id.value_counts()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "1892     134\n",
        "2035     121\n",
        "2041     104\n",
        "26504    101\n",
        "617       95\n",
        "2073      92\n",
        "2         91\n",
        "899       88\n",
        "625       80\n",
        "1077      77\n",
        "670       73\n",
        "57        72\n",
        "1867      70\n",
        "18967     62\n",
        "400       61\n",
        "...\n",
        "31357    1\n",
        "414      1\n",
        "674      1\n",
        "682      1\n",
        "374      1\n",
        "39261    1\n",
        "80       1\n",
        "342      1\n",
        "294      1\n",
        "786      1\n",
        "882      1\n",
        "37132    1\n",
        "254      1\n",
        "946      1\n",
        "32768    1\n",
        "Length: 2054, dtype: int64"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "titles = data.news_title"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#titles = data.ix[data.main_related_company_id==target_companyID]['news_title']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "titles.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "0         \u70e4\u9e2d\uff1a150\u5e74\u820c\u5c16\u8bb0\u5fc6\n",
        "1          \u738b\u5e9c\u4e95\u5e97\u6e05\u660e\u7545\u4eab\u7f8e\u5473\n",
        "2    \u5e74\u62a5\u4e1a\u7ee9\u5927\u589e204% \u62df\u63a8\u9ad8\u9001\u8f6c\n",
        "3    2013\u5e74\u51c0\u5229\u589e6\u6210 \u521b\u5386\u53f2\u65b0\u9ad8\n",
        "4       \u62df\u8f6c\u8ba9\u65d7\u4e0b\u4e50\u7535\u5929\u5a01\u7845\u4e1a\u80a1\u6743\n",
        "Name: news_title, dtype: object"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "publishTime = data.news_publish_time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#publishTime = data.ix[data.main_related_company_id==target_companyID]['news_publish_time']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# data.main_related_company_id.value_counts()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1323
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# titles = titles.apply(lambda x: x.replace(u'\u957f\u56ed',''))\n",
      "# titles = titles.apply(lambda x: x.replace(u'\u957f\u56ed\u96c6\u56e2',''))\n",
      "# # titles = titles.apply(lambda x: x.replace(u'\u65b0\u534e\u7f51',''))\n",
      "# titles = titles.apply(lambda x: x.replace(u'\uff1a',''))\n",
      "# titles = titles.apply(lambda x: x.replace(u':',''))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1324
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "titles.to_csv('/root/data/classifyNewsTitle/'+str(target_companyID)+'.txt', encoding='utf-8')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 111
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# stop and anotation"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 112
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#titles = pd.read_csv('/root/data/classifyNewsTitle/'+str(target_companyID)+'_anotated.txt', encoding='utf-8')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 113
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#true_base = titles['category']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 114
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#titles = titles['title']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 115
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# #ansj segemnt\n",
      "# \"\"\"\n",
      "# wrapper for java tools\n",
      "\n",
      "# Run start_jvm with classpaths first, you must ensure all need jars are supplied\n",
      "# \"\"\"\n",
      "# import jpype\n",
      "# import os\n",
      "# class AnsjSplitter:\n",
      "#     \"\"\" Wrapper for ansj\n",
      "\n",
      "#     Needed jars:\n",
      "#     ansj_seg-xxx.jar\n",
      "#     tree_split-xxx.jar\n",
      "#     \"\"\"\n",
      "#     def __init__(self, dict_path=None):\n",
      "#         \"\"\" Parameters:\n",
      "#         dict_path: path of userLibrary\n",
      "#         \"\"\"\n",
      "#         self.AnsjConfig = jpype.JClass('org.ansj.util.MyStaticValue')\n",
      "#         self.AnsjConfig.ambiguityLibrary = None\n",
      "#         self.AnsjConfig.userLibrary = dict_path\n",
      "#         self.methods = {\n",
      "#             'base': jpype.JClass('org.ansj.splitWord.analysis.BaseAnalysis'),\n",
      "#             'to': jpype.JClass('org.ansj.splitWord.analysis.ToAnalysis'),\n",
      "#             'nlp': jpype.JClass('org.ansj.splitWord.analysis.NlpAnalysis'),\n",
      "#         }\n",
      "#     def split(self, txt, method = 'to'):\n",
      "#         \"\"\" split txt using method\n",
      "#         method can be \"base\", \"to\", or \"nlp\"\n",
      "#         \"\"\"\n",
      "#         if method not in self.methods:\n",
      "#             raise RuntimeError('no such method for ansj')\n",
      "#         raw_res = self.methods[method].parse(txt).toArray()\n",
      "#         res = [(x.offe, x.offe+len(x.realName), x.nature.natureStr) for x in raw_res]\n",
      "#         return res\n",
      "\n",
      "# def start_jvm(class_path_list):\n",
      "#     \"\"\" Start jvm with class_path_list\n",
      "#     \"\"\"\n",
      "#     jpype.startJVM(jpype.getDefaultJVMPath(), '-ea', \"-Djava.class.path=%s\" % ':'.join(class_path_list))\n",
      "# def shutdown_jvm():\n",
      "#     \"\"\" shutdown jvm\n",
      "#     \"\"\"\n",
      "#     jpyte.shutdownJVM()\n",
      "# import glob\n",
      "# start_jvm(glob.glob('/root/jar_lib/*'))\n",
      "# ansj = AnsjSplitter('/root/data/userLibrary.txt')\n",
      "# def my_tokenizer(s):\n",
      "#     ll = ansj.split(s)\n",
      "#     res = []\n",
      "#     for tt in ll:\n",
      "#         if (tt[2]!=u'null') and (tt[2]!=u'p') and (tt[2]!=u'u'):\n",
      "#             res.append(s[tt[0]: tt[1]])\n",
      "#     return res"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def n_gram_tokenizer(s):\n",
      "    s = s.replace(' ','')\n",
      "    res = []\n",
      "    for ind in range(len(s)-1):\n",
      "        res.append(s[ind:ind+2])\n",
      "    return res"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cv = CountVectorizer(analyzer=n_gram_tokenizer)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cntContentRes = cv.fit_transform(titles)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cntContentRes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "<13265x53219 sparse matrix of type '<type 'numpy.int64'>'\n",
        "\twith 195536 stored elements in Compressed Sparse Column format>"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wordsDF=pickle.load(file('titleWordsDf.ser'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feature = cv.get_feature_names();"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wdDF_subset = []\n",
      "for wd in feature:\n",
      "    if wordsDF.has_key(wd):\n",
      "        wdDF_subset.append(wordsDF[wd])\n",
      "    else:\n",
      "        wdDF_subset.append(0)\n",
      "wdIdf = []\n",
      "maxDF = max(wdDF_subset)\n",
      "for x in wdDF_subset:\n",
      "    if x > 0:\n",
      "        wdIdf.append(log(maxDF/x))\n",
      "    else:\n",
      "        wdIdf.append(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "publishTimeMap = publishTime.apply(lambda x: (x- x.now()).days)\n",
      "publishTimeMap = publishTimeMap.values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "titlesVal = titles.values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wdIdfArr = array(wdIdf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def countSim(arr1, arr2):\n",
      "    return sum(wdIdfArr[(arr1>0) & (arr2>0)])  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "news_count = len(titles)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feature = array(feature)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "simFileList[abs(publishTimeMap.values[simFileList])<30]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'simFileList' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-34-3a828296218b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msimFileList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpublishTimeMap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msimFileList\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: name 'simFileList' is not defined"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "publishTimeMap.values[simFileList]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 58,
       "text": [
        "array([-58, -58, -78, ..., -69, -58, -38])"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "simFileList[abs(publishTimeMap.values[simFileList])<7]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 46,
       "text": [
        "array([], dtype=int64)"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.sparse.csc import csc_matrix\n",
      "b = csc_matrix((news_count, news_count))\n",
      "for st in xrange(news_count):\n",
      "    \n",
      "    l1 = len(titlesVal[st])\n",
      "    ar1 = cntContentRes[st].toarray()[0]\n",
      "    words = n_gram_tokenizer(titlesVal[st])\n",
      "    simFileList = array([],dtype='int')\n",
      "    for wd in words:\n",
      "        simFileList = np.concatenate((simFileList,cntContentRes.T[feature==wd].nonzero()[1]))\n",
      "    simFileList = array(list(set(simFileList)))\n",
      "    \n",
      "    simFileList = simFileList[simFileList>st]\n",
      "    if(len(simFileList) != 0):\n",
      "        simFileList = simFileList[abs(publishTimeMap[simFileList]-publishTimeMap[st])<7]\n",
      "    if st%100==0:\n",
      "        print st\n",
      "        sys.stdout.flush()\n",
      "    for i in simFileList:\n",
      "        l2 = len(titlesVal[i])\n",
      "        ar2 = cntContentRes[i].toarray()[0]\n",
      "#        index = [(ar1==1) & (ar2==1)]\n",
      "        if publishTimeMap[st] > publishTimeMap[i]:\n",
      "            delt = (publishTimeMap[st] - publishTimeMap[i])\n",
      "        else:\n",
      "            delt = (publishTimeMap[i] - publishTimeMap[st])\n",
      "        b[st,i] = (countSim(ar1,ar2)/log(min(l1,l2))*(1-delt*0.1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "100\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "200\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "300\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "400\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "500\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "600\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "700\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "800\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "900\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1100\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1200\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1300\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1400\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1500\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1600\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1700\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1800\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1900\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2100\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2200\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2300\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2400\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2500\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2600\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2700\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2800\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2900\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3100\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3200\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3300\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3400\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3500\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3600\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3700\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3800\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3900\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4100\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4200\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4300\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4400\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4500\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4600\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4700\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4800\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4900\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5100\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5200\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5300\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5400\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5500\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5600\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5700\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5800\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5900\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "6000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "6100\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "6200\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "6300\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "6400\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/root/env1/local/lib/python2.7/site-packages/scipy/sparse/compressed.py:728: SparseEfficiencyWarning: Changing the sparsity structure of a csc_matrix is expensive. lil_matrix is more efficient.\n",
        "  SparseEfficiencyWarning)\n",
        "\n",
        "KeyboardInterrupt\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "Traceback (most recent call last):\n",
        "  File \"/root/env1/local/lib/python2.7/site-packages/IPython/kernel/zmq/ipkernel.py\", line 389, in execute_request\n    shell.run_cell(code, store_history=store_history, silent=silent)\n",
        "  File \"/root/env1/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2743, in run_cell\n    self.events.trigger('post_execute')\n",
        "  File \"/root/env1/local/lib/python2.7/site-packages/IPython/core/events.py\", line 74, in trigger\n    def trigger(self, event, *args, **kwargs):\n",
        "KeyboardInterrupt\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%debug"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "> \u001b[1;32m/root/env1/local/lib/python2.7/site-packages/scipy/sparse/sparsetools/csr.py\u001b[0m(677)\u001b[0;36mget_csr_submatrix\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32m    676 \u001b[1;33m    \"\"\"\n",
        "\u001b[0m\u001b[1;32m--> 677 \u001b[1;33m  \u001b[1;32mreturn\u001b[0m \u001b[0m_csr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_csr_submatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[0m\u001b[1;32m    678 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[0m\n"
       ]
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ipdb> q\n"
       ]
      }
     ],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def calSimilarity(title1, title2):\n",
      "    sp1 = n_gram_tokenizer(title1)\n",
      "    sp2 = n_gram_tokenizer(title2)\n",
      "    sim = 0.0\n",
      "    for wd1 in sp1:\n",
      "        for wd2 in sp2:\n",
      "            try:\n",
      "                tmp = model2.similarity(wd1,wd2)\n",
      "                if tmp > 0.2:\n",
      "                    sim = sim + tmp\n",
      "            except Exception: \n",
      "                pass\n",
      "    return sim"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "titles"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_sample = len(titles)\n",
      "title_dist = [[0]*n_sample for i in range(n_sample)]\n",
      "for idx in range(n_sample):\n",
      "    for idy in range(n_sample):\n",
      "        title_dist[idx][idy] = calSimilarity(titles.values[idx],titles.values[idy])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cluster import AffinityPropagation\n",
      "af = AffinityPropagation(affinity = 'precomputed',preference=2).fit(array(title_dist))\n",
      "cluster_centers_indices = af.cluster_centers_indices_\n",
      "labels = af.labels_\n",
      "res = pd.DataFrame({'title':titles.values, 'category': labels, 'center':0})\n",
      "\n",
      "for ind in cluster_centers_indices:\n",
      "    res['center'].ix[ind] = 1\n",
      "res = res.sort('category')\n",
      "res.to_csv('/root/data/classifyNewsTitle/'+str(target_companyID)+'clsResWithWord2VecPropagation.txt',encoding = 'utf-8',sep = '\\t',index = False)\n",
      "\n",
      "\n",
      "# n_clusters_ = len(cluster_centers_indices)\n",
      "# print('Estimated number of clusters: %d' % n_clusters_)\n",
      "# print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
      "# print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
      "# print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
      "# print(\"Adjusted Rand Index: %0.3f\"\n",
      "#       % metrics.adjusted_rand_score(labels_true, labels))\n",
      "# print(\"Adjusted Mutual Information: %0.3f\"\n",
      "#       % metrics.adjusted_mutual_info_score(labels_true, labels))\n",
      "# print(\"Silhouette Coefficient: %0.3f\"\n",
      "#       % metrics.silhouette_score(X, labels, metric='sqeuclidean'))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cluster import AffinityPropagation\n",
      "af = AffinityPropagation(affinity = 'precomputed', preference=2).fit(array(b))\n",
      "cluster_centers_indices = af.cluster_centers_indices_\n",
      "labels = af.labels_\n",
      "res = pd.DataFrame({'title':titles.values, 'category': labels, 'center':0})\n",
      "\n",
      "for ind in cluster_centers_indices:\n",
      "    res['center'].ix[ind] = 1\n",
      "res = res.sort('category')\n",
      "res.to_csv('/root/data/classifyNewsTitle/'+str(target_companyID)+'clsResWithWeightedIdfPropagation.txt',encoding = 'utf-8',sep = '\\t',index = False)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_clusters_ = len(cluster_centers_indices)\n",
      "catDistMat = [[0]*n_clusters_ for i in range(n_clusters_)]  \n",
      "n_sample = len(titles)\n",
      "for idx in range(n_sample):\n",
      "    for idy in range(n_sample):\n",
      "        catDistMat[labels[idx]][labels[idy]] = catDistMat[labels[idx]][labels[idy]] + title_dist[idx][idy]\n",
      "catDistMat = array(catDistMat)/float(n_sample)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "publishTimeByCate = [[] for i in range(n_clusters_)]\n",
      "for ind in range(n_sample):\n",
      "    publishTimeByCate[labels[ind]].append(publishTime.iloc[ind])\n",
      "midTime = [t[len(t)/2] for t in publishTimeByCate]\n",
      "timeDistMat = [[0]*n_clusters_ for i in range(n_clusters_)] \n",
      "for inx in range(n_clusters_):\n",
      "    for iny in range(n_clusters_):\n",
      "        if midTime[inx] > midTime[iny]:\n",
      "            timeDistMat[inx][iny] = (midTime[inx] - midTime[iny]).days\n",
      "        else:\n",
      "            timeDistMat[inx][iny] = (midTime[iny] - midTime[inx]).days\n",
      "        if timeDistMat[inx][iny] > 9 :\n",
      "            timeDistMat[inx][iny] = 9\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for inx in range(n_clusters_):\n",
      "    for iny in range(n_clusters_):\n",
      "        catDistMat[inx][iny] = catDistMat[inx][iny]*(1-timeDistMat[inx][iny]*0.1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cluster import AffinityPropagation\n",
      "centerTitles = titles.values[cluster_centers_indices]\n",
      "af = AffinityPropagation(affinity = 'precomputed', preference=2).fit(array(catDistMat))\n",
      "cluster_centers_indices_sub = af.cluster_centers_indices_\n",
      "sublabels = af.labels_\n",
      "res = pd.DataFrame({'centerTitles':centerTitles, 'category': sublabels, 'center':0})\n",
      "\n",
      "for ind in cluster_centers_indices_sub:\n",
      "    res['center'].ix[ind] = 1\n",
      "res = res.sort('category')\n",
      "res.to_csv('/root/data/classifyNewsTitle/'+str(target_companyID)+'clsResWithWeightedIdfPropagation.txt',mode = 'a', encoding = 'utf-8',sep = '\\t',index = False)\n",
      "for ind in range(len(titles)):\n",
      "    labels[ind] = sublabels[labels[ind]]\n",
      "\n",
      "res = pd.DataFrame({'titles':titles.values, 'category': labels, 'center':0})\n",
      "for ind in cluster_centers_indices_sub:\n",
      "    res['center'].ix[cluster_centers_indices[ind]] = 1\n",
      "res = res.sort('category')\n",
      "res.to_csv('/root/data/classifyNewsTitle/'+str(target_companyID)+'clsResWithWeightedIdfAndWord2VecPropagation.txt',mode = 'a', encoding = 'utf-8',sep = '\\t',index = False)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Kmeans and EucilidDistance\n",
      "# from sklearn.cluster import KMeans\n",
      "# estimator = KMeans(init='k-means++', n_clusters=8)\n",
      "# estimator.fit(cntContentRes)\n",
      "# pd.DataFrame({'title':titles, 'category': estimator.labels_}).sort('category').to_csv('/root/data/classifyNewsTitle/clsResWithEucilidDistKmeans'+str(target_companyID)+'.txt',encoding = 'utf-8',sep = '\\t',index = False)\n",
      "# n_clusters_ = 5\n",
      "# labels = estimator.labels_\n",
      "# print('Estimated number of clusters: %d' % n_clusters_)\n",
      "# print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
      "# print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
      "# print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
      "# print(\"Adjusted Rand Index: %0.3f\"\n",
      "#       % metrics.adjusted_rand_score(labels_true, labels))\n",
      "# print(\"Adjusted Mutual Information: %0.3f\"\n",
      "#       % metrics.adjusted_mutual_info_score(labels_true, labels))\n",
      "# print(\"Silhouette Coefficient: %0.3f\"\n",
      "#       % metrics.silhouette_score(X, labels, metric='sqeuclidean'))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#import word2vec"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#word2vec.word2vec('/root/data/corpus/result.txt', '/root/data/word2vec_model/model.bin', window=20, size=200, verbose=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#model = word2vec.load('/root/data/word2vec_model/model.bin')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#pd.Series(model.cosine('\u6d89\u5acc')['\u6d89\u5acc'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import gensim.models import word2vec\n",
      "# >>> import logging\n",
      "# >>> logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
      "# >>> # load up unzipped corpus from http://mattmahoney.net/dc/text8.zip\n",
      "# >>> sentences = word2vec.Text8Corpus('/root/data/corpus/result.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# model = word2vec.Word2Vec(sentences, size=20, workers=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import gensim.models\n",
      "# model2 = gensim.models.word2vec.Word2Vec.load_word2vec_format('/root/data/word2vec_model/model.bin', binary=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# model2.similarity(u'\u6d89\u5acc',u'\u8fdd\u89c4')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# from gensim.models import word2vec\n",
      "# >>> import logging\n",
      "# >>> logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
      "# >>> # load up unzipped corpus from http://mattmahoney.net/dc/text8.zip\n",
      "# >>> sentences = word2vec.Text8Corpus('/root/data/corpus/result.txt')\n",
      "# >>> # train the skip-gram model; default window=5\n",
      "# >>> model = word2vec.Word2Vec(sentences, size=200)\n",
      "# >>> # ... and some hours later... just as advertised...\n",
      "# >>> model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
      "# [('queen', 0.5359965)]\n",
      " \n",
      "# >>> # pickle the entire model to disk, so we can load&resume training later\n",
      "# >>> model.save('/tmp/text8.model')\n",
      "# >>> # store the learned weights, in a format the original C tool understands\n",
      "# >>> model.save_word2vec_format('/tmp/text8.model.bin', binary=True)\n",
      "# >>> # or, import word weights created by the (faster) C word2vec\n",
      "# >>> # this way, you can switch between the C/Python toolkits easily\n",
      "# >>> model = word2vec.Word2Vec.load_word2vec_format('/tmp/vectors.bin', binary=True)\n",
      " \n",
      "# >>> # \"boy\" is to \"father\" as \"girl\" is to ...?\n",
      "# >>> model.most_similar(['girl', 'father'], ['boy'], topn=3)\n",
      "# [('mother', 0.61849487), ('wife', 0.57972813), ('daughter', 0.56296098)]\n",
      "# >>> more_examples = [\"he his she\", \"big bigger bad\", \"going went being\"]\n",
      "# >>> for example in more_examples:\n",
      "# ...     a, b, x = example.split()\n",
      "# ...     predicted = model.most_similar([x, b], [a])[0][0]\n",
      "# ...     print \"'%s' is to '%s' as '%s' is to '%s'\" % (a, b, x, predicted)\n",
      "# 'he' is to 'his' as 'she' is to 'her'\n",
      "# 'big' is to 'bigger' as 'bad' is to 'worse'\n",
      "# 'going' is to 'went' as 'being' is to 'was'\n",
      " \n",
      "# >>> # which word doesn't go with the others?\n",
      "# >>> model.doesnt_match(\"breakfast cereal dinner lunch\".split())\n",
      "# 'cereal'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}